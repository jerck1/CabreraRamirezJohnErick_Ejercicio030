{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.datasets\n",
    "import sklearn.preprocessing\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.datasets\n",
    "import torchvision.models\n",
    "import torchvision.transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 8, 8)\n"
     ]
    }
   ],
   "source": [
    "numeros = sklearn.datasets.load_digits()\n",
    "imagenes = numeros['images']  # Hay 1797 digitos representados en imagenes 8x8\n",
    "n_imagenes = len(imagenes)\n",
    "X = imagenes.copy()\n",
    "Y = numeros['target']\n",
    "print(np.shape(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1797, 1, 8, 8])\n"
     ]
    }
   ],
   "source": [
    "inputs = torch.autograd.Variable(torch.Tensor(X).float()).unsqueeze(1)\n",
    "print(inputs.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1008x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# muestra algunos ejemplos\n",
    "\n",
    "n_items = inputs.data.size()[0]\n",
    "random_items = np.random.choice(np.arange(n_items), 5)\n",
    "\n",
    "plt.figure(figsize=(14,5))\n",
    "for i in range(5):\n",
    "    plt.subplot(1,5,i+1)\n",
    "    item  = random_items[i]\n",
    "    plt.imshow(inputs[item][0].detach().numpy())\n",
    "    plt.title(Y[item])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1797, 1, 8, 8])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normaliza\n",
    "mean = inputs.mean(dim=0)\n",
    "std = inputs.std(dim=0)\n",
    "std[std==0]=1.0\n",
    "\n",
    "for i in range(len(inputs)):\n",
    "    inputs[i] = (inputs[i])/std\n",
    "np.shape(inputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define el autoencoder\n",
    "Variamos los parámetrosk1,k2,k3 con k3: espacio latente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Autoencoder(torch.nn.Module):\n",
    "    def __init__(self,k1, k2, k3):\n",
    "        super(Autoencoder,self).__init__()\n",
    "        #k1=16; k2=8; k3=4\n",
    "        self.k1=k1\n",
    "        self.k2=k2\n",
    "        self.k3=k3\n",
    "        self.encoder = torch.nn.Sequential(\n",
    "            torch.nn.Conv2d(1, k1, kernel_size=3),\n",
    "            torch.nn.Conv2d(k1,k2,kernel_size=3),\n",
    "            torch.nn.Conv2d(k2,k3,kernel_size=3))\n",
    "        self.decoder = torch.nn.Sequential(\n",
    "            torch.nn.ConvTranspose2d(k3,k2,kernel_size=3),\n",
    "            torch.nn.ConvTranspose2d(k2,k1,kernel_size=3),\n",
    "            torch.nn.ConvTranspose2d(k1,1,kernel_size=3))\n",
    "    def forward(self,x,k1, k2, k3):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (2): Conv2d(4, 2, kernel_size=(3, 3), stride=(1, 1))\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): ConvTranspose2d(2, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ConvTranspose2d(4, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (2): ConvTranspose2d(16, 1, kernel_size=(3, 3), stride=(1, 1))\n",
      "  )\n",
      ")\n",
      "1627\n",
      "Autoencoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (2): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): ConvTranspose2d(4, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ConvTranspose2d(4, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (2): ConvTranspose2d(16, 1, kernel_size=(3, 3), stride=(1, 1))\n",
      "  )\n",
      ")\n",
      "1773\n",
      "Autoencoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (2): Conv2d(4, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): ConvTranspose2d(6, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ConvTranspose2d(4, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (2): ConvTranspose2d(16, 1, kernel_size=(3, 3), stride=(1, 1))\n",
      "  )\n",
      ")\n",
      "1919\n",
      "Autoencoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (2): Conv2d(4, 8, kernel_size=(3, 3), stride=(1, 1))\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): ConvTranspose2d(8, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ConvTranspose2d(4, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (2): ConvTranspose2d(16, 1, kernel_size=(3, 3), stride=(1, 1))\n",
      "  )\n",
      ")\n",
      "2065\n",
      "Autoencoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (2): Conv2d(4, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): ConvTranspose2d(10, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ConvTranspose2d(4, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (2): ConvTranspose2d(16, 1, kernel_size=(3, 3), stride=(1, 1))\n",
      "  )\n",
      ")\n",
      "2211\n",
      "Autoencoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): Conv2d(16, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (2): Conv2d(4, 12, kernel_size=(3, 3), stride=(1, 1))\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): ConvTranspose2d(12, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ConvTranspose2d(4, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (2): ConvTranspose2d(16, 1, kernel_size=(3, 3), stride=(1, 1))\n",
      "  )\n",
      ")\n",
      "2357\n",
      "Autoencoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (2): Conv2d(8, 2, kernel_size=(3, 3), stride=(1, 1))\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): ConvTranspose2d(2, 8, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ConvTranspose2d(8, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (2): ConvTranspose2d(16, 1, kernel_size=(3, 3), stride=(1, 1))\n",
      "  )\n",
      ")\n",
      "2931\n",
      "Autoencoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (2): Conv2d(8, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): ConvTranspose2d(4, 8, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ConvTranspose2d(8, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (2): ConvTranspose2d(16, 1, kernel_size=(3, 3), stride=(1, 1))\n",
      "  )\n",
      ")\n",
      "3221\n",
      "Autoencoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (2): Conv2d(8, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): ConvTranspose2d(6, 8, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ConvTranspose2d(8, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (2): ConvTranspose2d(16, 1, kernel_size=(3, 3), stride=(1, 1))\n",
      "  )\n",
      ")\n",
      "3511\n",
      "Autoencoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (2): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1))\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): ConvTranspose2d(8, 8, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ConvTranspose2d(8, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (2): ConvTranspose2d(16, 1, kernel_size=(3, 3), stride=(1, 1))\n",
      "  )\n",
      ")\n",
      "3801\n",
      "Autoencoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (2): Conv2d(8, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): ConvTranspose2d(10, 8, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ConvTranspose2d(8, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (2): ConvTranspose2d(16, 1, kernel_size=(3, 3), stride=(1, 1))\n",
      "  )\n",
      ")\n",
      "4091\n",
      "Autoencoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): Conv2d(16, 8, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (2): Conv2d(8, 12, kernel_size=(3, 3), stride=(1, 1))\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): ConvTranspose2d(12, 8, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ConvTranspose2d(8, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (2): ConvTranspose2d(16, 1, kernel_size=(3, 3), stride=(1, 1))\n",
      "  )\n",
      ")\n",
      "4381\n",
      "Autoencoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): Conv2d(16, 12, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (2): Conv2d(12, 2, kernel_size=(3, 3), stride=(1, 1))\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): ConvTranspose2d(2, 12, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ConvTranspose2d(12, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (2): ConvTranspose2d(16, 1, kernel_size=(3, 3), stride=(1, 1))\n",
      "  )\n",
      ")\n",
      "4235\n",
      "Autoencoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): Conv2d(16, 12, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (2): Conv2d(12, 4, kernel_size=(3, 3), stride=(1, 1))\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): ConvTranspose2d(4, 12, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ConvTranspose2d(12, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (2): ConvTranspose2d(16, 1, kernel_size=(3, 3), stride=(1, 1))\n",
      "  )\n",
      ")\n",
      "4669\n",
      "Autoencoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): Conv2d(16, 12, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (2): Conv2d(12, 6, kernel_size=(3, 3), stride=(1, 1))\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): ConvTranspose2d(6, 12, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ConvTranspose2d(12, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (2): ConvTranspose2d(16, 1, kernel_size=(3, 3), stride=(1, 1))\n",
      "  )\n",
      ")\n",
      "5103\n",
      "Autoencoder(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): Conv2d(16, 12, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (2): Conv2d(12, 8, kernel_size=(3, 3), stride=(1, 1))\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): ConvTranspose2d(8, 12, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): ConvTranspose2d(12, 16, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (2): ConvTranspose2d(16, 1, kernel_size=(3, 3), stride=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# entrenamiento\n",
    "# inicializa modelo, loss y optimizador\n",
    "k1=16\n",
    "k2_tot=[4,8,12]\n",
    "#k3=4\n",
    "latent_space=np.linspace(2.,12.,6)\n",
    "n_c_tot=[]\n",
    "loss_tot=[]\n",
    "for k2 in k2_tot:\n",
    "    for k3 in latent_space:\n",
    "        num_epochs = 100\n",
    "        model = Autoencoder(k1,int(k2),int(k3))\n",
    "        print(model)\n",
    "        distance = torch.nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.005, weight_decay=1E-4)\n",
    "        loss_list = [] \n",
    "        for epoch in range(num_epochs):\n",
    "            output = model(inputs,k1,8,4)\n",
    "            loss = distance(output, inputs)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            loss_list.append(loss.item())\n",
    "            #print('epoch [{}/{}], loss:{:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "        loss_tot.append(loss_list[-1])\n",
    "        #type(loss.item())\n",
    "        n_c=0\n",
    "        for m in model.parameters():\n",
    "            n_c+=m.flatten().size()[0]\n",
    "        n_c_tot.append(n_c)\n",
    "        print(n_c)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_tot_2=np.reshape(loss_tot,(len(k2_tot),len(latent_space)))\n",
    "n_c_2=np.reshape(n_c_tot,(len(k2_tot),len(latent_space)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(k2_tot)):\n",
    "    plt.scatter(n_c_2[i],loss_tot_2[i],label=\"k2={}\".format(i+1))\n",
    "plt.xlabel(\"N_c\")\n",
    "plt.ylabel(\"Final Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(k2_tot)):\n",
    "    plt.scatter(latent_space,loss_tot_2[i],label=\"k2={}\".format(i+1))\n",
    "plt.xlabel(\"latent space dimension\")\n",
    "plt.ylabel(\"Final Loss\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(k2_tot)):\n",
    "    plt.scatter(n_c_2[i],latent_space,label=\"k2={}\".format(i+1))\n",
    "plt.xlabel(\"N_c\")\n",
    "plt.ylabel(\"latent space dimension\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discusión\n",
    "* En la primera gráfica se muestra la pérdida final (para la última iteración de épocas) versus el número de parámetros. Es posible ver que al aumentar el número de parámetros, para una dimensión de la capa intermedia k2 constante, hace que la pérdida diminuya. Además si aumentamos la dimensionalidad de k2 ocurre un corrimiento de la gráfica, es decir, se aumenta el número de parámetros pero la pérdida sigue siendo la misma.\n",
    "* En la segunda gráfica se muestra la pérdida final (\"loss\") versus la dimensión del espacio latente. Se observa que al aumentar la dimensión del espacio latente disminuye el \"loss\" y que la convergencia hacia un valor mínimo de loss mejora cuando aumentamos el número la dimensionalidad de la capa intermedia k2, la cual es la dimensión de salida de la primera convolución y es la dimensión de entrada de la segunda convolución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
